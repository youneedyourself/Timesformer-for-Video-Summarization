{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from pathlib import Path\n",
    "from timesformer.models.vit import *\n",
    "from timesformer.datasets import utils as utils\n",
    "from timesformer.config.defaults import get_cfg\n",
    "from einops import rearrange, repeat, reduce\n",
    "import cv2\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "DEFAULT_MEAN = [0.45, 0.45, 0.45]\n",
    "DEFAULT_STD = [0.225, 0.225, 0.225]\n",
    "\n",
    "# convert video path to input tensor for model\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(DEFAULT_MEAN,DEFAULT_STD),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "])\n",
    "\n",
    "# convert the video path to input for cv2_imshow()\n",
    "transform_plot = transforms.Compose([\n",
    "    lambda p: cv2.imread(str(p),cv2.IMREAD_COLOR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    lambda x: rearrange(x*255, 'c h w -> h w c').numpy()\n",
    "])\n",
    "\n",
    "\n",
    "def get_frames(path_to_video, num_frames=8):\n",
    "  \"return a list of paths to the frames of sampled from the video\"\n",
    "  path_to_frames = list(path_to_video.iterdir())\n",
    "  path_to_frames.sort(key=lambda f: int(f.with_suffix('').name[-6:]))\n",
    "  assert num_frames <= len(path_to_frames), \"num_frames can't exceed the number of frames extracted from videos\"\n",
    "  if len(path_to_frames) == num_frames:\n",
    "    return(path_to_frames)\n",
    "  else:\n",
    "    video_length = len(path_to_frames)\n",
    "    seg_size = float(video_length - 1) / num_frames \n",
    "    seq = []\n",
    "    for i in range(num_frames):\n",
    "      start = int(np.round(seg_size * i))\n",
    "      end = int(np.round(seg_size * (i + 1)))\n",
    "      seq.append((start + end) // 2)\n",
    "      path_to_frames_new = [path_to_frames[p] for p in seq]\n",
    "    return(path_to_frames_new)\n",
    "\n",
    "def create_video_input(path_to_video):\n",
    "  \"create the input tensor for TimeSformer model\"\n",
    "  path_to_frames = get_frames(path_to_video)\n",
    "  frames = [transform(cv2.imread(str(p), cv2.IMREAD_COLOR)) for p in path_to_frames]\n",
    "  frames = torch.stack(frames, dim=0)\n",
    "  frames = rearrange(frames, 't c h w -> c t h w')\n",
    "  frames = frames.unsqueeze(dim=0)\n",
    "  return(frames)\n",
    "\n",
    "def show_mask_on_image(img, mask):\n",
    "    img = np.float32(img) / 255\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)\n",
    "    #return cv2.cvtColor(np.uint8(255 * cam), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def create_masks(masks_in, np_imgs):\n",
    "  masks = []\n",
    "  for mask, img in zip(masks_in, np_imgs):\n",
    "    mask= cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
    "    mask = show_mask_on_image(img, mask)\n",
    "    masks.append(mask)\n",
    "  return(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def space_only_attention_masks(attn_s):\n",
    "    attn_s = attn_s.mean(dim = 1)\n",
    "    # adding residual and renormalize \n",
    "    attn_s = attn_s + torch.eye(attn_s.size(-1))[None,...]\n",
    "    attn_s = attn_s / attn_s.sum(-1)[...,None]\n",
    "\n",
    "    attn_s = rearrange(attn_s, 't1 p1 p2 -> p1 p2 t1')\n",
    "    return attn_s\n",
    "\n",
    "class SpaceAttentionRollout():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.hooks = []\n",
    "\n",
    "    def get_attn_s(self, module, input, output):\n",
    "        self.space_attentions.append(output.detach().cpu())\n",
    "\n",
    "    def remove_hooks(self): \n",
    "        for h in self.hooks: \n",
    "            h.remove()\n",
    "    \n",
    "    def __call__(self, path_to_video):\n",
    "        input_tensor = create_video_input(path_to_video)\n",
    "        self.model.zero_grad()\n",
    "        self.space_attentions = []\n",
    "        self.attentions = []\n",
    "        for name, m in self.model.named_modules():\n",
    "            if 'attn.attn_drop' in name:\n",
    "                self.hooks.append(m.register_forward_hook(self.get_attn_s))\n",
    "\n",
    "        preds = self.model(input_tensor)\n",
    "        for h in self.hooks: \n",
    "            h.remove()\n",
    "        \n",
    "        print(\"space_attentions:\", len(self.space_attentions))\n",
    "        for attn_s in self.space_attentions:\n",
    "            print(space_only_attention_masks(attn_s).shape)\n",
    "            self.attentions.append(space_only_attention_masks(attn_s))\n",
    "\n",
    "        p,t = self.attentions[0].shape[0], self.attentions[0].shape[2]\n",
    "\n",
    "        print(\"self_att:\", self.attentions[0].shape)\n",
    "        print(\"p:\", p)\n",
    "        print(\"t:\", t)\n",
    "\n",
    "        result = torch.eye(p*t)\n",
    "\n",
    "        for attention in self.attentions:\n",
    "            attention = attention.unsqueeze(1).repeat(1, 8, 1, 1)\n",
    "            print(attention.shape)\n",
    "            attention = rearrange(attention, 'p1 t1 p2 t2-> (p1 t1) (p2 t2)')\n",
    "            result = torch.matmul(attention, result)\n",
    "            \n",
    "        mask = rearrange(result, '(p1 t1) (p2 t2) -> p1 t1 p2 t2', p1 = p, p2=p)\n",
    "        mask = mask.mean(dim=1)\n",
    "        mask = mask[0,1:,:]\n",
    "        print(\"mask:\", mask.shape)\n",
    "        print(mask.size(0))\n",
    "        width = int(mask.size(0)**0.5)\n",
    "        mask = rearrange(mask, '(h w) t -> h w t', w = width).numpy()\n",
    "        print(\"mask rearrange:\", mask.shape)\n",
    "        mask = mask / np.max(mask)\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = 'timesformer\\TimeSformer_spaceOnly_8x32_224.pyth'\n",
    "Path(model_file).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file('configs/SSv2/TimeSformer_spaceOnly_8x32_224.yaml')\n",
    "cfg.TRAIN.ENABLE = False\n",
    "cfg.TIMESFORMER.PRETRAINED_MODEL = model_file\n",
    "model = MODEL_REGISTRY.get('vit_base_patch16_224')(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vit_base_patch16_224(\n",
       "  (model): VisionTransformer(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=400, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "read_data = pd.read_csv('example_data\\kinetics\\kinetics_400_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinetics_labels = list(read_data['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kinetics_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_video = Path('example_data/74225/')\n",
    "path_to_video.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False):\n",
    "  np.random.seed(cfg.RNG_SEED)\n",
    "  torch.manual_seed(cfg.RNG_SEED)\n",
    "  model.eval();\n",
    "  pred = model(create_video_input(path_to_video)).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4680, 1.4605, 1.1831, 1.1716, 1.1499]])\n",
      "tensor([[ 31, 218, 325, 235,  37]])\n",
      "Prediction index 0: bowling                  , score: 1.468\n",
      "Prediction index 1: playing badminton        , score: 1.460\n",
      "Prediction index 2: somersaulting            , score: 1.183\n",
      "Prediction index 3: playing ice hockey       , score: 1.172\n",
      "Prediction index 4: brushing teeth           , score: 1.150\n"
     ]
    }
   ],
   "source": [
    "topk_scores, topk_label = torch.topk(pred, k=5, dim=-1)\n",
    "\n",
    "print(topk_scores)\n",
    "print(topk_label)\n",
    "\n",
    "for i in range(5):\n",
    "  pred_name = kinetics_labels[topk_label.squeeze()[i].item()]\n",
    "  print(f\"Prediction index {i}: {pred_name:<25}, score: {topk_scores.squeeze()[i].item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space_attentions: 12\n",
      "torch.Size([197, 197, 8])\n",
      "torch.Size([197, 197, 8])\n",
      "torch.Size([197, 197, 8])\n",
      "torch.Size([197, 197, 8])\n",
      "torch.Size([197, 197, 8])\n",
      "torch.Size([197, 197, 8])\n",
      "torch.Size([197, 197, 8])\n",
      "torch.Size([197, 197, 8])\n",
      "torch.Size([197, 197, 8])\n",
      "torch.Size([197, 197, 8])\n",
      "torch.Size([197, 197, 8])\n",
      "torch.Size([197, 197, 8])\n",
      "self_att: torch.Size([197, 197, 8])\n",
      "p: 197\n",
      "t: 8\n",
      "torch.Size([197, 8, 197, 8])\n",
      "torch.Size([197, 8, 197, 8])\n",
      "torch.Size([197, 8, 197, 8])\n",
      "torch.Size([197, 8, 197, 8])\n",
      "torch.Size([197, 8, 197, 8])\n",
      "torch.Size([197, 8, 197, 8])\n",
      "torch.Size([197, 8, 197, 8])\n",
      "torch.Size([197, 8, 197, 8])\n",
      "torch.Size([197, 8, 197, 8])\n",
      "torch.Size([197, 8, 197, 8])\n",
      "torch.Size([197, 8, 197, 8])\n",
      "torch.Size([197, 8, 197, 8])\n",
      "mask: torch.Size([196, 8])\n",
      "196\n",
      "mask rearrange: (14, 14, 8)\n"
     ]
    }
   ],
   "source": [
    "att_roll = SpaceAttentionRollout(model)\n",
    "masks = att_roll(path_to_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_imgs = [transform_plot(p) for p in get_frames(path_to_video)]\n",
    "masks_output = create_masks(list(rearrange(masks, 'h w t -> t h w')), np_imgs)\n",
    "cv2.imshow('', np.hstack(masks_output))\n",
    "cv2.imwrite('space_only_att.jpg', np.hstack(masks_output))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
