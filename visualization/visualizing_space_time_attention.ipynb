{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiyixuxu/TimeSformer/blob/main/visualizing_space_time_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkMqF4RNdboe"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HQtop8Xr48HK"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "from pathlib import Path\n",
        "from timesformer.models.vit import *\n",
        "from timesformer.datasets import utils as utils\n",
        "from timesformer.config.defaults import get_cfg\n",
        "from einops import rearrange, repeat, reduce\n",
        "import cv2\n",
        "#from google.colab.patches import cv2_imshow\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiTDjCG6wf3x"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "N8UIk_MFwn5M"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "DEFAULT_MEAN = [0.45, 0.45, 0.45]\n",
        "DEFAULT_STD = [0.225, 0.225, 0.225]\n",
        "\n",
        "# convert video path to input tensor for model\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(DEFAULT_MEAN,DEFAULT_STD),\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "])\n",
        "\n",
        "# convert the video path to input for cv2_imshow()\n",
        "transform_plot = transforms.Compose([\n",
        "    lambda p: cv2.imread(str(p),cv2.IMREAD_COLOR),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    lambda x: rearrange(x*255, 'c h w -> h w c').numpy()\n",
        "])\n",
        "\n",
        "\n",
        "def get_frames(path_to_video, num_frames=8):\n",
        "  \"return a list of paths to the frames of sampled from the video\"\n",
        "  path_to_frames = list(path_to_video.iterdir())\n",
        "  path_to_frames.sort(key=lambda f: int(f.with_suffix('').name[-6:]))\n",
        "  assert num_frames <= len(path_to_frames), \"num_frames can't exceed the number of frames extracted from videos\"\n",
        "  if len(path_to_frames) == num_frames:\n",
        "    return(path_to_frames)\n",
        "  else:\n",
        "    video_length = len(path_to_frames)\n",
        "    seg_size = float(video_length - 1) / num_frames \n",
        "    seq = []\n",
        "    for i in range(num_frames):\n",
        "      start = int(np.round(seg_size * i))\n",
        "      end = int(np.round(seg_size * (i + 1)))\n",
        "      seq.append((start + end) // 2)\n",
        "      path_to_frames_new = [path_to_frames[p] for p in seq]\n",
        "    return(path_to_frames_new)\n",
        "\n",
        "def create_video_input(path_to_video):\n",
        "  \"create the input tensor for TimeSformer model\"\n",
        "  path_to_frames = get_frames(path_to_video)\n",
        "  frames = [transform(cv2.imread(str(p), cv2.IMREAD_COLOR)) for p in path_to_frames]\n",
        "  frames = torch.stack(frames, dim=0)\n",
        "  frames = rearrange(frames, 't c h w -> c t h w')\n",
        "  frames = frames.unsqueeze(dim=0)\n",
        "  return(frames)\n",
        "\n",
        "def show_mask_on_image(img, mask):\n",
        "    img = np.float32(img) / 255\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam = heatmap + np.float32(img)\n",
        "    cam = cam / np.max(cam)\n",
        "    return np.uint8(255 * cam)\n",
        "    #return cv2.cvtColor(np.uint8(255 * cam), cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "def create_masks(masks_in, np_imgs):\n",
        "  # 8 (224, 224, 3)\n",
        "  # (8, 14, 14) \n",
        "  masks = []\n",
        "  for mask, img in zip(masks_in, np_imgs):\n",
        "    # (14, 14)\n",
        "    mask= cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
        "    mask = show_mask_on_image(img, mask)\n",
        "    masks.append(mask)\n",
        "  return(masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T+S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cKHAX0VhSXKX"
      },
      "outputs": [],
      "source": [
        "# export\n",
        "def combine_divided_attention(attn_t, attn_s):\n",
        "  ## time attention\n",
        "    # average time attention weights across heads\n",
        "  print(\"attn_t before:\", attn_t.shape) # attn_t before: torch.Size([196, 12, 8, 8])\n",
        "  attn_t = attn_t.mean(dim = 1) \n",
        "  print(\"attn_t after mean:\", attn_t.shape) #attn_t after mean: torch.Size([196, 8, 8])\n",
        " \n",
        "    # add cls_token to attn_t as an identity matrix since it only attends to itself \n",
        "  I = torch.eye(attn_t.size(-1)).unsqueeze(0) \n",
        "  attn_t = torch.cat([I,attn_t], 0) #attn_t after mean: torch.Size([197, 8, 8])\n",
        "    # adding identity matrix to account for skipped connection \n",
        "  attn_t = attn_t +  torch.eye(attn_t.size(-1))[None,...]\n",
        "    # renormalize\n",
        "  attn_t = attn_t / attn_t.sum(-1)[...,None]\n",
        "  print(\"att_T\", attn_t.shape) # att_T torch.Size([197, 8, 8])\n",
        "\n",
        "\n",
        "  ## space attention\n",
        "   # average across heads\n",
        "  print(\"attn_s before:\", attn_s.shape) # attn_s before: torch.Size([8, 12, 197, 197])\n",
        "  attn_s = attn_s.mean(dim = 1)\n",
        "  print(\"attn_s after mean:\", attn_s.shape) #attn_s after mean: torch.Size([8, 197, 197])\n",
        "   # adding residual and renormalize \n",
        "  attn_s = attn_s +  torch.eye(attn_s.size(-1))[None,...]\n",
        "  attn_s = attn_s / attn_s.sum(-1)[...,None]\n",
        "  print(\"att_S\", attn_s.shape) # att_S torch.Size([8, 197, 197])\n",
        "\n",
        "  ## combine the space and time attention\n",
        "  attn_ts = einsum('tpk, ktq -> ptkq', attn_s, attn_t)\n",
        "  print(\"att_TS 1: \", attn_ts.shape)  # att_TS 1:  torch.Size([197, 8, 197, 8])\n",
        "  \n",
        "  ## average the cls_token attention across the frames\n",
        "   # splice out the attention for cls_token\n",
        "  attn_cls = attn_ts[0,:,:,:] \n",
        "  print(\"attn_cls:\", attn_cls.shape) # attn_cls: torch.Size([8, 197, 8])\n",
        "   # average the cls_token attention and repeat across the frames\n",
        "  attn_cls_a = attn_cls.mean(dim=0)  # attn_cls_a after mean: torch.Size([197, 8])\n",
        "  attn_cls_a = repeat(attn_cls_a, 'p t -> j p t', j = 8)\n",
        "  print(\"attn_cls_a:\", attn_cls_a.shape) # attn_cls_a: torch.Size([8, 197, 8])\n",
        "\n",
        "   # add it back\n",
        "  attn_ts = torch.cat([attn_cls_a.unsqueeze(0),attn_ts[1:,:,:,:]],0)\n",
        "  print(\"att_TS 2: \", attn_ts.shape) #att_TS 2:  torch.Size([197, 8, 197, 8])\n",
        "  return(attn_ts)\n",
        "\n",
        "class DividedAttentionRollout():\n",
        "  def __init__(self, model, **kwargs):\n",
        "    self.model = model\n",
        "    self.hooks = []\n",
        "\n",
        "  def get_attn_t(self, module, input, output):\n",
        "    self.time_attentions.append(output.detach().cpu())\n",
        "  def get_attn_s(self, module, input, output):\n",
        "    self.space_attentions.append(output.detach().cpu())\n",
        "\n",
        "  def remove_hooks(self): \n",
        "    for h in self.hooks: h.remove()\n",
        "    \n",
        "  def __call__(self, path_to_video):\n",
        "    input_tensor = create_video_input(path_to_video)\n",
        "    self.model.zero_grad()\n",
        "    self.time_attentions = []\n",
        "    self.space_attentions = []\n",
        "    self.attentions = []\n",
        "    for name, m in self.model.named_modules():\n",
        "      if 'temporal_attn.attn_drop' in name:\n",
        "        self.hooks.append(m.register_forward_hook(self.get_attn_t))\n",
        "      elif 'attn.attn_drop' in name:\n",
        "        self.hooks.append(m.register_forward_hook(self.get_attn_s))\n",
        "\n",
        "    preds = self.model(input_tensor)\n",
        "    for h in self.hooks: h.remove()\n",
        "    \n",
        "    print(\"self_Att\", len(self.space_attentions)) # self_Att 12\n",
        "  \n",
        "    for attn_t,attn_s in zip(self.time_attentions, self.space_attentions):\n",
        "                                    # attn_ts = einsum('tpk, ktq -> ptkq', attn_s, attn_t) \n",
        "      print(\"attn_s\", attn_s.shape) # attn_s torch.Size([8, 12, 197, 197])\n",
        "      print(\"attn_t\", attn_t.shape) # attn_t torch.Size([196, 12, 8, 8])\n",
        "      \n",
        "      print(\"combined attention\", combine_divided_attention(attn_t,attn_s).shape) # combined attention torch.Size([197, 8, 197, 8])\n",
        "      self.attentions.append(combine_divided_attention(attn_t,attn_s))\n",
        "\n",
        "    print(\"list attentions after combine:\", len(self.attentions), self.attentions[0].shape) \n",
        "    # list attentions after combine: 12 torch.Size([197, 8, 197, 8])\n",
        "\n",
        "    p,t = self.attentions[0].shape[0], self.attentions[0].shape[1]\n",
        "    print(p) #197\n",
        "    print(t) #8\n",
        "    result = torch.eye(p*t)\n",
        "\n",
        "    print(\"Result\", result.shape) #torch.Size([1576, 1576])\n",
        "\n",
        "    for attention in self.attentions:\n",
        "      attention = rearrange(attention, 'p1 t1 p2 t2 -> (p1 t1) (p2 t2)')\n",
        "      print(\"att after rearrange\", attention.shape) # att after rearrange torch.Size([1576, 1576])\n",
        "      result = torch.matmul(attention, result) #torch.Size([1576, 1576])\n",
        "\n",
        "    mask = rearrange(result, '(p1 t1) (p2 t2) -> p1 t1 p2 t2', p1 = p, p2=p)\n",
        "    print(\"mask 1:\", mask.shape) #mask 1: torch.Size([197, 8, 197, 8])\n",
        "    mask = mask.mean(dim=1)\n",
        "    print(\"mask 2:\", mask.shape) #mask 2: torch.Size([197, 197, 8])\n",
        "    mask = mask[0,1:,:]\n",
        "    print(\"mask 3:\", mask.shape) #mask 3: torch.Size([196, 8])\n",
        "    width = int(mask.size(0)**0.5)\n",
        "    print(\"width:\", width) # width: 14\n",
        "    mask = rearrange(mask, '(h w) t -> h w t', w = width).numpy()\n",
        "    print(\"mask 4:\", mask.shape) # mask 4: (14, 14, 8)\n",
        "    mask = mask / np.max(mask)\n",
        "    print(\"mask 5\", mask.shape) # mask 5 (14, 14, 8)\n",
        "    return(mask)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H20-Np38qhVM"
      },
      "source": [
        "# load the pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3L75p3W-mLn"
      },
      "source": [
        "download the pre-trainde model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip6HavtbtmRh",
        "outputId": "2d233350-2589-4c90-9545-22cd66fedb5c"
      },
      "outputs": [],
      "source": [
        "#! wget https://dl.dropboxusercontent.com/s/tybhuml57y24wpm/TimeSformer_divST_8_224_SSv2.pyth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H1oq7vF-pxD"
      },
      "source": [
        "load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORsqV7ZlsEJB",
        "outputId": "5c8d609d-ad53-4a61-cb51-fa912ffa2d77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_file = 'timesformer/TimeSformer_divST_8_224_SSv2.pyth'\n",
        "Path(model_file).exists()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HskcnYcS1jSo"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file('configs/SSv2/TimeSformer_divST_8_224.yaml')\n",
        "cfg.TRAIN.ENABLE = False\n",
        "cfg.TIMESFORMER.PRETRAINED_MODEL = model_file\n",
        "model = MODEL_REGISTRY.get('vit_base_patch16_224')(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_a7HTsI-2DI"
      },
      "source": [
        "read the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y4AvpeqOjk2D"
      },
      "outputs": [],
      "source": [
        "with open('example_data/labels.json') as f:\n",
        "  ssv2_labels = json.load(f)\n",
        "ssv2_labels = list(ssv2_labels.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6Hia6Ph-51o"
      },
      "source": [
        "inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9G1vctJ7kxv",
        "outputId": "9ccf414f-3751-4fa8-8f9b-22ed70d7824f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path_to_video = Path('example_data/74225/')\n",
        "path_to_video.exists()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gyojK4Iy7QFL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "with torch.set_grad_enabled(False):\n",
        "  np.random.seed(cfg.RNG_SEED)\n",
        "  torch.manual_seed(cfg.RNG_SEED)\n",
        "  model.eval();\n",
        "  pred = model(create_video_input(path_to_video)).cpu().detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEphQOJE8NnU",
        "outputId": "6e61af10-247c-4e36-bfb4-e26d0d367704"
      },
      "outputs": [],
      "source": [
        "topk_scores, topk_label = torch.topk(pred, k=5, dim=-1)\n",
        "for i in range(5):\n",
        "  pred_name = ssv2_labels[topk_label.squeeze()[i].item()]\n",
        "  print(f\"Prediction index {i}: {pred_name:<25}, score: {topk_scores.squeeze()[i].item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyGmROAg9pOO"
      },
      "source": [
        "# visualizing the learned space-time attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci8UT8yB_ErR"
      },
      "source": [
        "Create a `DividedAttentionRollout` object (`att_roll`) and call it to get a mask for a given video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3qCY_CS9rL-"
      },
      "outputs": [],
      "source": [
        "att_roll = DividedAttentionRollout(model)\n",
        "masks = att_roll(path_to_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Dfas1gR_VM_"
      },
      "source": [
        "plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np_imgs = [transform_plot(p) for p in get_frames(path_to_video)]\n",
        "print(len(np_imgs), np_imgs[0].shape)\n",
        "print(masks.shape)\n",
        "masks_output = create_masks(list(rearrange(masks, 'h w t -> t h w')), np_imgs)\n",
        "cv2.imshow('', np.hstack(masks_output))\n",
        "#cv2.imwrite('original_imgs.jpg', np.hstack(np_imgs))\n",
        "cv2.imwrite('divided_space_time_att.jpg', np.hstack(masks_output))\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "color_list = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "def visualize_colormap(mask):\n",
        "  min_val, max_val = np.min(mask), np.max(mask)  # Get min and max values of the mask\n",
        "  print(min_val, max_val)\n",
        "  gradient = np.linspace(min_val, max_val, 101)[:, None]  # Create a gradient from min to max\n",
        "\n",
        "  _, ax = plt.subplots(figsize=(1, 10))  # Create a new figure with a custom size\n",
        "\n",
        "  ax.imshow(gradient, aspect='auto', cmap=plt.get_cmap('jet'), origin='lower')  # Display with the 'jet' colormap\n",
        "  ax.set_xticks([])  # Hide x-axis ticks\n",
        "  ax.set_yticks(np.arange(0, 101, 20))  # Set y-axis ticks at regular intervals\n",
        "  ax.tick_params(axis='y', labelsize=40)  # Increase font size of y-axis labels\n",
        "  plt.show()  # Show the plot\n",
        "  plt.savefig('color_map.png')\n",
        "\n",
        "mask_values = masks.flatten()\n",
        "visualize_colormap(mask_values)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOtDwQEa7MFcIjh+elUfcY5",
      "collapsed_sections": [
        "H20-Np38qhVM"
      ],
      "include_colab_link": true,
      "name": "TimeSformer_rolled_attention.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
